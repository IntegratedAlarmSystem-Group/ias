package org.eso.ias.dasu

import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong, AtomicReference}
import java.util.concurrent.{ScheduledFuture, TimeUnit}
import java.util.{HashMap, Objects, Properties}
import org.eso.ias.asce.{AsceStates, ComputingElement}
import org.eso.ias.cdb.pojos.{AsceDao, DasuDao}
import org.eso.ias.cdb.topology.DasuTopology
import org.eso.ias.dasu.executorthread.ScheduledExecutor
import org.eso.ias.dasu.publisher.OutputPublisher
import org.eso.ias.dasu.subscriber.InputSubscriber
import org.eso.ias.logging.IASLogger
import org.eso.ias.types.*

import scala.jdk.javaapi.CollectionConverters
import scala.util.{Failure, Success, Try}

/**
 * The implementation of the DASU.
 * 
 * A DASU normally has 2 threads running:
 * - the automatic sending of the output when no new input arrives 
 *   or the output did not change
 * - a thread (with throttling) to process the inputs and generate the output
 * 
 * As of #248, the DASU processes the inputs to generate the output in a thread
 * that is immediately executed upon reception of the inputs. 
 * To avoid using 100% of CPU two consecutive generation of the output are delayed
 * by a throttling time interval. During that time the DASU continues to collect inputs.
 * 
 * @param dasuIdentifier       the identifier of the DASU
 * @param dasuDao              The CDB configuration of the DASU
 * @param outputPublisher      the publisher to send the output
 * @param inputSubscriber      the subscriber getting events to be processed
 * @param autoSendTimeInterval the time (seconds) to automatically resend the last calculated
 * @param validityThreshold    the max delay (secs) before declaring an input unreliable
 */
class DasuImpl (
    dasuIdentifier: Identifier,
    dasuDao: DasuDao,
    private val outputPublisher: OutputPublisher,
    private val inputSubscriber: InputSubscriber,
    autoSendTimeInterval: Integer,
    validityThreshold: Integer)
    extends Dasu(dasuIdentifier,autoSendTimeInterval,validityThreshold) {
  require(Option(dasuIdentifier).isDefined,"Invalid Identifier")
  require(Option(dasuDao).isDefined,"Invalid DASU CDB configuration")
  require(dasuIdentifier.idType==IdentifierType.DASU,"Invalid identifier type for DASU")
  require(Option(outputPublisher).isDefined,"Invalid output publisher")
  require(Option(inputSubscriber).isDefined,"Invalid input subscriber")
  require(Option(autoSendTimeInterval).isDefined && autoSendTimeInterval>0,"Invalid auto-send time interval")

  DasuImpl.logger.info("Building DASU [{}] with running id {}",id,dasuIdentifier.fullRunningID)

  /**
   * The configuration of the ASCEs that run in the DASU
   */
  val asceDaos: List[AsceDao] = CollectionConverters.asScala(dasuDao.getAsces).toList

  // Are there ASCEs assigned to this DASU?
  require(dasuDao.getAsces.size()>0,"No ASCE found for DASU "+id)
  DasuImpl.logger.info("DASU [{}]: will load and run {} ASCEs",id,""+dasuDao.getAsces.size())

  // The ID of the output generated by the DASU
  val dasuOutputId: String = dasuDao.getOutput.getId
  DasuImpl.logger.info("Output of DASU [{}]: [{}]",id,dasuOutputId)

  // Build the topology
  val dasuTopology: DasuTopology = new DasuTopology(
      id,
      dasuOutputId,
      asceDaos)
  DasuImpl.logger.debug("DASU [{}]: topology built",id)
  DasuImpl.logger.debug(dasuTopology.toString())

  // Instantiate the ASCEs
  val asces: Map[String, ComputingElement[?]] = {
    val addToMapFunc = (m: Map[String, ComputingElement[?]], asce: AsceDao) => {
      val propsForAsce = new Properties()
      asce.getProps.forEach(p => propsForAsce.setProperty(p.getName, p.getValue))
      m + (asce.getId -> ComputingElement(asce,dasuIdentifier,validityThreshold,propsForAsce))
    }
    asceDaos.foldLeft(Map[String,ComputingElement[?]]())(addToMapFunc)
  }


  DasuImpl.logger.info("DASU [{}] ASCEs loaded: [{}]",id, asces.keys.mkString(", "))

  // Activate the ASCEs
  val ascesInitedOk: Boolean =asces.valuesIterator.map(asce => asce.initialize()).forall(s => s==AsceStates.InputsUndefined)
  assert(ascesInitedOk,"At least one ASCE did not pass the initialization phase")
  DasuImpl.logger.info("DASU [{}]: ASCEs initialized",id)

  // The ASCE that produces the output of the DASU
  val idOfAsceThatProducesTheOutput: String = {
    val idOpt=dasuTopology.asceProducingOutput(dasuOutputId)
    require(idOpt.isDefined && !idOpt.isEmpty,"ASCE producing output not found")
    idOpt.get
  }
  DasuImpl.logger.info("The output [{}] of the DASU [{}] is produced by [{}] ASCE",dasuOutputId,id,idOfAsceThatProducesTheOutput)

  /** The ASCE that produces the output */
  val asceThatProducesTheOutput: ComputingElement[?] = asces(idOfAsceThatProducesTheOutput)

  /**
    * Values that have been received in input from plugins or other DASUs (BSDB)
    * and not yet processed by the ASCEs
    *
    * This map must be taken synchronized because it is accessed by several threads
    */
  val notYetProcessedInputs: java.util.Map[String,IASValue[?]] = new HashMap[String,IASValue[?]]()

  /**
    * The fullRuning Ids of the received inputs
    *
    * This map must be taken synchronized because it is accessed by several threads
    */
  val fullRunningIdsOfInputs: java.util.Map[String,String] = new HashMap[String,String]()

  /**
   *  The last calculated output by ASCEs
   */
  val lastCalculatedOutput = new AtomicReference[Option[IASValue[?]]](None)

  /**
   *  The last sent output and validity
   *  this is sent again if no new inputs arrives and the autoSendTimerTask is running
   *  and the validity did not change since the last sending
   */
  val lastSentOutputAndValidity = new AtomicReference[Option[Tuple2[InOut[?], IasValidity]]](None)

  /**
   *  The point in time when the output has been sent to the
   *  BSDB either due to new inputs or auto-refresh
   *
   *  It is better to initialize at the actual timestamp
   *  for the calculation of the throttling in inputsReceived
   */
  val lastSentTime = new AtomicLong(System.currentTimeMillis())

  /** The thread executor service */
  val scheduledExecutor = new ScheduledExecutor(id)

  /** True if the automatic re-sending of the output has been enabled */
  val timelyRefreshing = new AtomicBoolean(false)

  /** True if the DASU has been started */
  val started = new AtomicBoolean(false)

  /**
   * The task to delay the generation the output
   * when new inputs must be processed
   * 
   * It is defined only when a task is running or scheduled to run in a near future.
   */
  val throttlingTask = new AtomicReference[Option[ScheduledFuture[?]]](None)

  /**
    * The point in time when the DASU started calculating the output for the last time
    *
    * Together with calcEndTime can be used to calculate how long does the DASU take 
    * to generate the output
    */
  val calcStartTime = new AtomicLong(0)

  /**
   * The point in time when the DASU finished calculating the output for the last time.
   *
   * It is used to schedulate the next generation of the output against the throttling time
   *
   * Together with calcStartTime can be used to calculate how long does the DASU take
   * to generate the output
   */
  val calcEndTime = new AtomicLong(0)

  /**
   * The Runnable to update the output when it is delayed by the throttling
   */
  val delayedUpdateTask: Runnable = new Runnable {
    override def run() = {
        DasuImpl.logger.debug("DASU [{}] running the throttling task to calc the output",id)
        calcStartTime.set(System.currentTimeMillis())
        Try(updateAndPublishOutput()) match {
          case Failure(exception) =>
            DasuImpl.logger.error("Exception caught in DASU [{}] while producing the output with throttling",
              id,
              exception)
          case _ =>
        }
        calcEndTime.set(System.currentTimeMillis())
        throttlingTask.set(None) // Reset the throttling task

        // Comments inputsReceived explains why this call is needed at this point
        scheduleNextOutputCalculation()
      }
  }

  /**
   * The point in time of the last time when 
   * the output has been generated and published
   */
  val lastUpdateTime = new AtomicLong(0)

  /**
   *  The task that timely send the last computed output
   *  when no new inputs arrived: initially disabled, must be enabled
   *  invoking enableAutoRefreshOfOutput.
   *
   *  If a new output is generated before this time interval elapses,
   *  this task is delayed of the duration of autoSendTimeInterval msecs.
   */
  val autoSendTimerTask: AtomicReference[ScheduledFuture[?]] = new AtomicReference[ScheduledFuture[?]]()

  /** Closed: the DASU does not process inputs */
  val closed = new AtomicBoolean(false)

  DasuImpl.logger.debug("DASU [{}]: initializing the publisher", id)

  val outputPublisherInitialized: Try[Unit] = outputPublisher.initializePublisher()
  outputPublisherInitialized match {
    case Failure(f) => DasuImpl.logger.error("DASU [{}] failed to initialize the publisher: NO output will be produced", id,f)
    case Success(_) => DasuImpl.logger.info("DASU [{}] publisher successfully initialized",id)
  }
  DasuImpl.logger.debug("DASU [{}] initializing the subscriber", id)

  val inputSubscriberInitialized: Try[Unit] = inputSubscriber.initializeSubscriber()
  inputSubscriberInitialized match {
    case Failure(f) => DasuImpl.logger.error("DASU [{}] failed to initialize the subscriber: NO input will be processed", id,f)
    case _ => DasuImpl.logger.info("DASU [{}] subscriber successfully initialized",id)
  }

  /** The generator of statistics */
  val statsCollector = new DasuStatistics(id)

  DasuImpl.logger.info("DASU [{}] built", id)

  /**
   * Reschedule the auto send task if the output is generated before
   * the autoSendTimeInterval elapses.
   */
  private def rescheduleAutoSendTimerTask(): Unit = synchronized {
    val autoSendIsEnabled = timelyRefreshing.get()
    if (autoSendIsEnabled) {

      val lastTimerScheduledFeature: Option[ScheduledFuture[?]] = Option(autoSendTimerTask.get)
      lastTimerScheduledFeature.foreach(_.cancel(false))
      val runnable = new Runnable {
          /** The task to refresh the output when no new inputs have been received */
          override def run(): Unit = {
            Try(asceThatProducesTheOutput.output.value.foreach(_ => publishOutput(calcOutputValidity()))) match {
              case Failure(exception) => DasuImpl.logger.error("Exception caught publishing output {} of DASU [{}]",
                dasuOutputId,id,exception)
              case _ =>
            }
          }
      }

      val newTask=scheduledExecutor.scheduleWithFixedDelay(
          runnable,
          autoSendTimeInterval.toLong,
          autoSendTimeInterval.toLong,
          TimeUnit.SECONDS)
      autoSendTimerTask.set(newTask)
    }
  }

  /**
   * Propagates the inputs received from the BSDB to each of the ASCEs
   * in the DASU generating the output of the entire DASU.
   *
   * This method runs after the throttling time interval elapses.
   * All the iasios collected in the time interval will be passed to the first level of the ASCEs
   * and up till the last ASCE that generates the output of the DASU itself.
   * Each ASCE runs the TF and produces another output to be propagated to the next level.
   * The last level is the one that produces the output of the DASU
   * to be sent to the BSDB.
   *
   * @param iasios the IASIOs received from the BDSB in the last time interval
   * @return the IASIO to send back to the BSDB
   */
  private def propagateIasios(iasios: Set[IASValue[?]]): Option[IASValue[?]] = {

      // Updates one ASCE i.e. runs its TF passing the inputs
      // and returns the output of the ASCE
      def updateOneAsce(asceId: String, asceInputs: Set[IASValue[?]]): Option[IASValue[?]] = {


        val requiredInputs = dasuTopology.inputsOfAsce(asceId)
        val inputs: Set[IASValue[?]] = asceInputs.filter( p => requiredInputs.contains(p.id))

        if (inputs.nonEmpty) {
          // Get the ASCE with the given ID
          val asceOpt: Option[ComputingElement[?]] = asces.get(asceId)
          assert(asceOpt.isDefined,"ASCE "+asceId+" NOT found!")

          asceOpt.flatMap(asce => asce.update(inputs).asInstanceOf[(Option[InOut[?]], AsceStates.State)].
            _1.map(inOut => inOut.toIASValue()))
        } else {
          None
        }

      }

      // Run the TFs of all the ASCEs in one level
      // Returns the inputs plus all the outputs produced by the ACSEs
      def updateOneLevel(asces: Set[String], levelInputs: Set[IASValue[?]]): Set[IASValue[?]] = {

        asces.foldLeft(levelInputs) (
          (s: Set[IASValue[?]], id: String ) => {
            updateOneAsce(id, levelInputs).map( v => s+v).getOrElse(s)
            })
      }

      if (!closed.get) {
        val outputs = dasuTopology.levels.foldLeft(iasios){ (s: Set[IASValue[?]], ids: Set[String]) => s ++ updateOneLevel(ids, s) }
        outputs.find(_.id==dasuOutputId).map(_.updateProdTime(System.currentTimeMillis()))
      } else {
        None
      }
  }

  /**
   * New inputs have been received from the BSDB: the recalculation of the output
   * is performed by a thread: the method returns immediately after scheduling the recalculation
   *
   * This method is not invoked while automatically re-sending the last computed value.
   * Such value is in fact stored into lastSentOutput and does not trigger
   * a recalculation by the DASU. 
   * 
   * This method gets the inputs received from the BSDB and calculate the output running 
   * a thread.
   * The calculation of the output is delayed if the throttling is in place by scheduling
   * a task to be run after the end of the throttling window. If the throttling is not in place,
   * the output is calculated concurrently by running a task immediately.
   * 
   * This method only schedules the thread to recalculate the output when new inputs are received
   * and the thread has not been already scheduled or is running.
   * 
   * There is one case that is not covered here:
   * - the thread is running and new inputs arrive before it terminates
   * - no new inputs arrive
   * This case is  corner case the likely will never happens when many inputs are continously produced.
   * However if it happens the inputs recived in this situation will never be processsed
   * (they will only as soon as a new inputs arrives).
   * For this reason, when the thread that calculates the new output terminates, it checks 
   * if there are new inputs and in that case it schedules a new execution of the output calculation.
   *
   * @param iasios the inputs received
   * @see InputsListener
   */
  override def inputsReceived(iasios: Iterable[IASValue[?]]): Unit = synchronized {
    assert(iasios.nonEmpty)
    DasuImpl.logger.debug(s"DASU [$id] received ${iasios.size} inputs.")

    def acceptIasValue(value: IASValue[?]): Boolean = {
      assert(Option(value).isDefined)
      assert(value.productionTStamp.isPresent,"Undefined production timestamp for "+value.toString)

      // Accept the value if
      //  * its ID is the ID of an input
      //  * its timetsamp is newer that that already in the map of inputs to process
      getInputIds().contains(value.id)

      val valueFromMap: Option[IASValue[?]] = Option(notYetProcessedInputs.get(value.id))
      valueFromMap.map (v => {

        val valueTstamp = value.productionTStamp.get
        assert(v.productionTStamp.isPresent, "Missing production timestamp for value from map: "+value.toString)
        val tstampOfValueInMap = v.productionTStamp.get()

        valueTstamp>=tstampOfValueInMap
      }).getOrElse(true) // Not in map: accept the value


    }

    // Merge the inputs with the buffered ones to keep only the last updated values
    // The scheduled task clears the buffered inputs (in updateAndPublishOutput)
    notYetProcessedInputs.synchronized {
      iasios.filter( acceptIasValue(_)).foreach(iasio => {
        fullRunningIdsOfInputs.synchronized { fullRunningIdsOfInputs.put(iasio.id, iasio.fullRunningId) }

        notYetProcessedInputs.put(iasio.id,iasio)
      })
    }
    scheduleNextOutputCalculation() // Schedule the next output calculation
  }

  /**
    * Schedule the next recalculation of the output based on 
    * - the presence of new inputs
    * - the point in time when the last calculation ended and the throttling time.
    * 
    * No recalulcalation is scheduled if there are no new inputs to process.
    * The delay depends on the time when the last calculation ended and the throttling time.
    */
  def scheduleNextOutputCalculation(): Unit = synchronized {
    // If the calculation of the output is already scheduled or running or there are no new inputs, do not schedule it again
    if (throttlingTask.get().isEmpty && hasInputsToProcess) { 
      val now = System.currentTimeMillis()
      // Schedule the output calculation
      val delay = if (now >= calcEndTime.get() + throttling) 0 else calcEndTime.get() + throttling - now
      val schedFeature = scheduledExecutor.schedule(delayedUpdateTask, delay, TimeUnit.MILLISECONDS)
      throttlingTask.set(Some(schedFeature))
      DasuImpl.logger.debug(s"DASU [$id] scheduled the next output calculation in ${delay} msecs.")
    } else {
      DasuImpl.logger.debug(s"DASU [$id] does not schedule the next output calculation: throttling task already running or no inputs to process")
    }
    
  }

  /**
   * Publish the last calculated output to the BSDB with the given validity.
   *
   * This method can be called by the automatic sending and by a change in the output
   * and blindly publish the last calculated output.
   *
   * @param actualValidity the validity
   */
  private def publishOutput(actualValidity: Validity): Unit = {

    val currentOutput = asceThatProducesTheOutput.output

    currentOutput.value.foreach(_ => {
      lastSentTime.set(System.currentTimeMillis())
      val iasioToSend = currentOutput.updateSentToBsdbTStamp(lastSentTime.get)
      val iasValueToSendWithDepIds = fullRunningIdsOfInputs.synchronized {
        iasioToSend.toIASValue().updateFullIdsOfDependents(fullRunningIdsOfInputs.values)
      }
      val iasValueToSend = iasValueToSendWithDepIds.updateValidity(actualValidity)

      lastSentOutputAndValidity.set(Some(iasioToSend,actualValidity))
      outputPublisher.publish(iasValueToSend)
    })
  }

  /**
    * Calculate the validity of the output depending on its last update time.
    * The DASU consider that the IASIO is valid if it has been generated
    * resh validityThreshold milliseconds before.
    */
  def calcOutputValidity(): Validity = {

    // The output can be undefined if the DASU tries to publish
    // before it is updated for the first time by the auto refresh task
    lastCalculatedOutput.get match {
      case None =>  // The output can be not defined if the DASU tries to publish before it is updated
        Validity(IasValidity.UNRELIABLE)
      case Some(lastOutput) =>
        assert(lastOutput.productionTStamp.isPresent, "IASIO has no production timestamp "+lastOutput.toString)

        val thresholdTStamp = System.currentTimeMillis() - validityThresholdMillis

        val iasioTstamp = lastOutput.productionTStamp.get()

        val validityByTime= if (iasioTstamp<thresholdTStamp) {
          Validity(IasValidity.UNRELIABLE)
        } else {
          Validity(IasValidity.RELIABLE)
        }

        // The validity of the output calculated by the ASCE
        val validityByAsce = Validity(lastOutput.iasValidity)
        Validity.minValidity(Set(validityByTime,validityByAsce))
    }

  }

  /**
   * Update the output with the set of inputs collected so far
   * and send the output to the BSDB.
   *
   * The method delegates the calculation to propapgateIasios
   */
  private def updateAndPublishOutput(): Unit = {

    val startTime = System.currentTimeMillis()
    // Converts the inputs from the synchronized java map into a immutable scala Set
    val inputsFromMap: Set[IASValue[?]] = notYetProcessedInputs.synchronized {
      val temp = CollectionConverters.asScala(notYetProcessedInputs.values).toSet
      notYetProcessedInputs.clear()
      temp
    }

    lastCalculatedOutput.set(propagateIasios(inputsFromMap))
    DasuImpl.logger.debug(s"DASU [$id] calculated output")
    val endTime = System.currentTimeMillis()

    lastUpdateTime.set(endTime)


    // Publish the value only if the output has been produced
    lastCalculatedOutput.get.foreach( output => {
      DasuImpl.logger.debug("DASU [{}] calculated output [{}] with validity {}",
        id,
        output.id,
        output.iasValidity)

      // The validity of the output
      //
      // The output has been calculated right now so this step is not really needed
      val outputValidity = calcOutputValidity()

      // Do we really need to send the output immediately?
      // If it did not change then it will be sent by the auto-send
      val prevSentOutputAndValidity = lastSentOutputAndValidity.get()

      assert(prevSentOutputAndValidity.forall(_._1.value.isDefined))

      if (prevSentOutputAndValidity.isEmpty ||
          prevSentOutputAndValidity.get._2!=outputValidity.iasValidity ||
          prevSentOutputAndValidity.get._1.value.get!= output.value ||
          prevSentOutputAndValidity.get._1.mode!=output.mode) {
        publishOutput(outputValidity)
        rescheduleAutoSendTimerTask()
        statsCollector.updateStats(endTime-startTime)
      }
    })
  }

  /**
    * Check if there are inputs to process 
    */
  def hasInputsToProcess: Boolean = {
    notYetProcessedInputs.synchronized {
      notYetProcessedInputs.size() > 0
    }
  }

  /**
   * Check if the a task for processing inputs has been scheduled
   *
   * @return true if a task for processing inputs has been scheduled
   */
  def hasScheduledTask: Boolean = {
    throttlingTask.get().isDefined
  }

  /**
   * Check if the new output must be sent to the BSDB
   * i.e if its mode or value or validity has changed
   *
   * @param oldOutput the last output sent to the BSDB
   * @param oldValidity the last validity sent to the BSDB
   * @param newOutput the new output
   * @param newValidity the new validity
   * @return true if the new output changed from the last sent output 
   *              and must be sent to the BSDB and false otherwise
   *
   */
  def mustSendOutput(
      oldOutput: InOut[?],
      oldValidity: Validity,
      newOutput: InOut[?],
      newValidity: Validity): Boolean = {
    oldValidity!=newValidity ||
    oldOutput.value!=newOutput.value ||
    oldOutput.mode!=newOutput.mode ||
    oldOutput.props!=newOutput.props ||
    oldOutput.idsOfDependants!=newOutput.idsOfDependants
  }

  /**
   *  Start getting events from the inputs subscriber
   *  to produce the output
   */
  def start(): Try[Unit] = {
    val alreadyStarted = started.getAndSet(true)
    if (!alreadyStarted) {
      DasuImpl.logger.debug("DASU [{}] starting", id)
      statsCollector.start()
      inputSubscriberInitialized.map(_ => inputSubscriber.startSubscriber(this, dasuTopology.inputsIds))
    } else {
      Failure(new Exception("DASU already started"))
    }
  }

  /**
   * Enable/disable the automatic update of the output
   * in case no new inputs arrive.
   *
   * Most likely, the value of the output remains the same 
   * while the validity could change.
   */
  def enableAutoRefreshOfOutput(enable: Boolean): Unit = synchronized {
    val alreadyEnabled = timelyRefreshing.getAndSet(enable)
    (enable, alreadyEnabled) match {
      case (true, true) =>
        DasuImpl.logger.warn("DASU [{}]: automatic refresh of output already ative",id)
      case (true, false) =>
        rescheduleAutoSendTimerTask()
        DasuImpl.logger.info("DASU [{}]: automatic send of output enabled at intervals of {} secs (aprox)",id, autoSendTimeInterval.toString)
      case (false , _) =>
        val oldTask: Option[ScheduledFuture[?]] = Option(autoSendTimerTask.getAndSet(null))
        oldTask.foreach(task => {
          task.cancel(false)
          DasuImpl.logger.info("DASU [{}]: automatic send of output disabled",id)
        })
    }
  }

  /**
   * Release all the resources before exiting
   */
  override def cleanUp(): Unit = {
    val alreadyClosed = closed.getAndSet(true)
    if (alreadyClosed) {
      DasuImpl.logger.warn("DASU [{}]: already cleaned up!", id)
    } else {
      DasuImpl.logger.info("DASU [{}]: releasing resources", id)
      DasuImpl.logger.debug("DASU [{}]: stopping statistics collector", id)
      statsCollector.cleanUp()
      DasuImpl.logger.debug("DASU [{}]: stopping the auto-refresh of the output", id)
      Try(enableAutoRefreshOfOutput(false))
      DasuImpl.logger.debug("DASU [{}]: releasing the subscriber", id)
      Try(inputSubscriber.cleanUpSubscriber())
      DasuImpl.logger.debug("DASU [{}]: releasing the publisher", id)
      Try(outputPublisher.cleanUpPublisher())
      DasuImpl.logger.info("DASU [{}]: cleaned up",id)
    }
  }

  /** @return the IDs of the inputs of the DASU */
  def getInputIds(): Set[String] = dasuTopology.inputsIds

  /** @return the IDs of the ASCEs running in the DASU  */
  def getAsceIds(): Set[String] = asces.keys.toSet

  /** @return the IDs of the of the inputs of the ASCE with the given ID */
  def getInputsOfAsce(id: String) = dasuTopology.inputsOfAsce(id)

  /**
   * ACK the alarm if the ASCE that produces it runs in this DASU.
   *
   * The DASU delegates the acknowledgment to the ASCE that produces the alarm.
   *
   * This function checks if the id of the DASU in the alarm identifier matches with its own identifier
   * and if the ASCE (again from the Identifier of the alarm) is actually running in this DASU before
   * forwarding the ack to the ASCE.
   *
   * @param alarmIdentifier the identifier of the alarm to ACK
   * @return true if the alarm has been ACKed, false otherwise
   * @see See [[org.eso.ias.asce.ComputingElement.ack()]]
   */
  override def ack(alarmIdentifier: Identifier): Boolean = {
    require(Objects.nonNull(alarmIdentifier))
    require(alarmIdentifier.idType==IdentifierType.IASIO) // Only IASIOs can be ACKed

    DasuImpl.logger.info("DASU [{}]: going to ACK {}", id, alarmIdentifier.fullRunningID)

    // Check if the ID of the DASU in the IASIO matches with the identifier of this DASU
    // The DASU in the identifier can be missing if, for example, the IASIO is produced by a plugin
    val dasuId: Option[String] = alarmIdentifier.getIdOfType(IdentifierType.DASU)
    if (!dasuId.isDefined) {
      DasuImpl.logger.warn("DASU [{}]: cannot ACK as the IASIO {} has no DASU in the identifier", id, alarmIdentifier.fullRunningID)
      return false
    }
    if (dasuId.get!=id) {
      DasuImpl.logger.warn("DASU [{}]: DASU mismatch in identifier (expected {}, found {}) ACKing {}",
        id,
        dasuId.get,
        id,
        alarmIdentifier.fullRunningID)
      return false
    } else {
      DasuImpl.logger.debug("DASU [{}]: this is the DASU in the identifier of the alarm {} to ack", id, alarmIdentifier.id)
    }


    // Check if this DASU really produces the alarm
    val alarmId = alarmIdentifier.getIdOfType(IdentifierType.IASIO)
    require(alarmId.isDefined, s"Invalid IASIO Identifier without IASIO id: [${alarmIdentifier.fullRunningID}]")
    if (alarmId.get!=dasuOutputId) {
      DasuImpl.logger.warn("DASU [{}] cannot ACK: this DASU does not produce [{}] but {}",
        id,
        alarmId.get,
        dasuOutputId)
      return false
    }
    DasuImpl.logger.debug("DASU [{}]: the alarm {} matches with the output of this DASU", id, alarmIdentifier.id)

    // Get ASCE whose TF produces the alarm to ACK
    // The ASCE in the identifier can be missing if, for example, the IASIO is produced by a plugin
    val asceId = alarmIdentifier.getIdOfType(IdentifierType.ASCE)
    if (!asceId.isDefined) {
      DasuImpl.logger.warn("DASU [{}] cannot ACK: the IASIO {} is not produced by an ASCE",id, alarmIdentifier.fullRunningID)
      return false
    }
    if (asceId.get!=idOfAsceThatProducesTheOutput) {
      DasuImpl.logger.warn("DASU [{} cannot ACK: the IASIO {} is not produced by the proper ASCE (expected {} but was {})",
        id,
        alarmIdentifier.fullRunningID,
        idOfAsceThatProducesTheOutput,
        asceId.get)
      return false
    }

    DasuImpl.logger.debug("DASU [{}]: the alarm {} is produced by the ASCE [{}] that runs in DASU",
      id,
      asceId.get,
      alarmIdentifier.id)

    // All checks passed: ack the alarm
    val acked = asces(asceId.get).ack()
    if (acked) {
      DasuImpl.logger.debug("DASU [{}]: send the ACKed alarm {} to BSD", id, alarmId.get)
      publishOutput(calcOutputValidity())
    }
    acked
  }
}

object DasuImpl {
  
  /**
   * Factory method to build a DasuImpl
   * 
   * @param dasuDao: the configuration of the DASU from the CDB
   * @param supervidentifier: the identifier of the supervisor that runs the dasu
   * @param outputPublisher: the producer to send outputs of DASUs to the BSDB
   * @param inputSubscriber: the consumer to get values from the BSDB
   * @param autoSendTimeInterval refresh rate (msec) to automatically send the output
   *                             when no new inputs have been received
   * @param validityThreshold the max delay (secs) before declaring an input unreliable
   */
  def apply(
    dasuDao: DasuDao, 
    supervidentifier: Identifier,
    outputPublisher: OutputPublisher,
    inputSubscriber: InputSubscriber,
    autoSendTimeInterval: Integer,
    validityThreshold: Integer): DasuImpl = {
    
    
    require(Option(dasuDao).isDefined)
    require(Option(supervidentifier).isDefined)
    require(Option(outputPublisher).isDefined)
    require(Option(inputSubscriber).isDefined)
    require(Option(autoSendTimeInterval).isDefined)
    require(Option(validityThreshold).isDefined)
    
    val dasuId = dasuDao.getId
    
    val dasuIdentifier = new Identifier(dasuId,IdentifierType.DASU,supervidentifier)
    
    new DasuImpl(
        dasuIdentifier,
        dasuDao,
        outputPublisher,
        inputSubscriber,
        autoSendTimeInterval,
        validityThreshold)
  }

  /** The logger */
  private val logger = IASLogger.getLogger(this.getClass)
}

