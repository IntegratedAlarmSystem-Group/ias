package org.eso.ias.dasu

import org.ias.prototype.logging.IASLogger
import org.eso.ias.cdb.CdbReader
import org.eso.ias.cdb.json.JsonReader
import org.eso.ias.cdb.json.CdbFiles
import org.eso.ias.cdb.json.CdbJsonFiles
import org.eso.ias.cdb.pojos.DasuDao
import org.eso.ias.dasu.topology.Topology

import scala.collection.JavaConverters
import org.eso.ias.cdb.pojos.DasuDao
import org.eso.ias.prototype.input.Identifier
import org.eso.ias.prototype.input.java.IdentifierType
import org.eso.ias.cdb.pojos.AsceDao
import org.eso.ias.prototype.compele.ComputingElement
import org.eso.ias.prototype.compele.ComputingElementState
import org.eso.ias.prototype.compele.AsceStates
import org.eso.ias.prototype.input.InOut
import org.eso.ias.kafkautils.SimpleStringProducer
import org.eso.ias.dasu.publisher.OutputPublisher
import org.eso.ias.prototype.input.java.IASValue
import org.eso.ias.prototype.input.JavaConverter
import org.eso.ias.dasu.executorthread.ScheduledExecutor
import scala.util.Try

/**
 * The Distributed Alarm System Unit or DASU.
 * 
 * The DASU, once notified of new inputs received from the BSDB (or other sources),
 * sends the IASIOs to the ASCEs to produce the output.
 * If no new inputs arrived the DASU generate the output anyhow to notify that the DASU is alive.
 * At a first glance it seems enough to check the validity of the last set of inputs to assign 
 * the validity of the output.
 * 
 * @constructor create a DASU with the given identifier
 * @param id is the identifier of the DASU
 * @param outputPublisher the publisher to send the output
 * @param cdbReader the CDB reader to get the configuration of the DASU from the CDB

 */
class Dasu(
    val id: String,
    private val outputPublisher: OutputPublisher,
    cdbReader: CdbReader)
    extends InputsListener {
  require(Option(outputPublisher).isDefined,"Invalid output publisher")
  
  /** The logger */
  private val logger = IASLogger.getLogger(this.getClass)
  
  /** The identifier of the DASU
   *  
   *  In this version it has no parent identifier because
   *  the Supervisor has not yet been implemented
   */
  val dasuIdentifier: Identifier = new Identifier(id,IdentifierType.DASU,None)
  
  logger.info("Reading CDB configuration of [{}] DASU",dasuIdentifier.toString())
  
  // Read configuration from CDB
  val dasuDao = {
    val dasuOptional = cdbReader.getDasu(id)
    require(dasuOptional.isPresent(),"DASU ["+id+"] configuration not found on cdb")
    dasuOptional.get
  }
  logger.debug("DASU [{}] configuration red from CDB",id)
  
  /**
   * The configuration of the ASCEs that run in the DASU
   */
  val asceDaos = JavaConverters.asScalaSet(dasuDao.getAsces).toList
  
  // Are there ASCEs assigned to this DASU?
  require(dasuDao.getAsces.size()>0,"No ASCE found for DASU "+id)
  logger.info("DASU [{}]: will load and run {} ASCEs",id,""+dasuDao.getAsces.size())
  
  // The ID of the output generated by the DASU
  val dasuOutputId = dasuDao.getOutput().getId()
  
  // Build the topology
  val dasuTopology: Topology = new Topology(
      id,
      dasuOutputId,
      asceDaos)
  logger.debug("DASU [{}]: topology built",id) 
  
  // Instantiate the ASCEs
  val asces: Map[String, ComputingElement[_]] = {
    val addToMapFunc = (m: Map[String, ComputingElement[_]], asce: AsceDao) => 
      m + (asce.getId -> ComputingElement(asce,dasuIdentifier,System.getProperties()))
    asceDaos.foldLeft(Map[String,ComputingElement[_]]())(addToMapFunc)
  }
  logger.info("DASU [{}] ASCEs loaded: [{}]",id, asces.keys.mkString(", "))
    
  // Activate the ASCEs
  val ascesInitedOk=asces.valuesIterator.map(asce => asce.initialize()).forall(s => s.actualState==AsceStates.Healthy)
  assert(ascesInitedOk,"At least one ASCE did not pass the initialization phase")
  logger.info("DASU [{}]: ASCEs initialized",id)
  
  // The ASCE that produces the output of the DASU
  val asceThatProducesTheOutput = dasuTopology.asceProducingOutput(dasuOutputId)
  require(asceThatProducesTheOutput.isDefined && !asceThatProducesTheOutput.isEmpty,"ASCE producing output not found")
  logger.info("The output [{}] of the DASU [{}] is produced by [{}] ASCE",dasuOutputId,id,asceThatProducesTheOutput.get)
  
  // Get the required refresh rate i.e. the rate to produce the output 
  // of the DASU that is given by the ASCE that produces such IASIO
  val refreshRate = asceDaos.find(_.getOutput.getId==dasuOutputId).map(asce => asce.getOutput().getRefreshRate())
  require(refreshRate.isDefined,"Refresh rate not found!")
  logger.info("The DASU [{}] produces the output [{}} at a rate of {}ms",id,dasuOutputId,""+refreshRate.get)
  
  /** The thread executor service */
  val scheduledExecutor = new ScheduledExecutor(id)
  
  /** 
   *  The helper to schedule the next refresh of the output
   *  when no inputs have been received
   */
  val timeScheduler = new TimeScheduler(dasuDao)
  
  // TODO: the rate to generate the output is taken from the 
  //       refresh rate of the IOASIO generated by the last ASCE of the DASU
  //       However cannot be greater to the shortest frame rate of the inputs 
  //       to update its validity
  logger.info("DASU [{}] built", id)
  
  /**
   * Propagates the inputs received from the BSDB to each of the ASCEs
   * in the DASU generating the output of the entire DASU.
   * 
   * THis method runs after the refresh time interval elapses.
   * All the iasios collected in the time interval will be passed to the first level of the ASCEs
   * and up till the last ASCE generates the output of the DASU itself.
   * Each ASCE runs the TF and produces another output to be propagated to the next level.
   * The last level is the only one ASCE that produces the output of the DASU
   * to be sent to the BSDB.
   * 
   * @param iasios the IASIOs received from the BDSB in the last time interval
   * @return the IASIO to send back to the BSDB
   */
  def propagateIasios(iasios: Set[IASValue[_]]): Option[IASValue[_]] = {
      
      // Updates one ASCE i.e. runs its TF passing the inputs
      // Return the output of the ASCE
      def updateOneAsce(asceId: String, iasios: Set[IASValue[_]]): Option[IASValue[_]] = {
        val requiredInputs = dasuTopology.inputsOfAsce(asceId)
        assert(requiredInputs.isDefined,"No inputs required by "+asceId)
        println("\tASCE required inputs "+requiredInputs.get.mkString(", "))
        println("\tAvailable inputs "+iasios.mkString(", "))
        val inputs: Set[IASValue[_]] = iasios.filter( p => requiredInputs.get.contains(p.id))
        println("\tAccepted inputs "+inputs.mkString(", "))
        val inOutOpt = asces.get(asceId).map(x => x.update(inputs)._1)
        inOutOpt.map(JavaConverter.inOutToIASValue(_))
      }
      
      // Run the TFs of all the ASCE of one level
      // Returns the inputs plus all the outputs produced by the ACSEs
      def updateOneLevel(asces: Set[String], iasios: Set[IASValue[_]]): Set[IASValue[_]] = {
        asces.foldLeft(iasios){ (s: Set[IASValue[_]], id: String) => s + updateOneAsce(id, iasios).get}
      }
      
      val outputs = dasuTopology.levels.foldLeft(iasios){ (s: Set[IASValue[_]], ids: Set[String]) => s ++ updateOneLevel(ids, iasios) }
      
      // TODO: update the validity
      outputs.find(_.id==dasuOutputId)
  }
  
  /**
   * Updates the output with the inputs received
   * 
   * @param iasios the inputs received
   * @see InputsListener
   */
  override def inputsReceived(iasios: Set[IASValue[_]]) {
    println("Updating with "+iasios.size+" inputs")
    val before = System.currentTimeMillis()
    val newOutput = propagateIasios(iasios)
    val executionTime = System.currentTimeMillis()-before
    
    newOutput.foreach( output => outputPublisher.publish(output) )
  }
}

object Dasu {
  /** The time interval to log statistics (minutes) */
  val DeafaultStatisticsTimeInterval = 10
  
  /** The name of the java property to set the statistics generation time interval */
  val StatisticsTimeIntervalPropName = "ias.dasu.stats.timeinterval"
  
  /** The actual time interval to log statistics (minutes) */
  val StatisticsTimeInterval: Int = {
    val prop = Option(System.getProperties.getProperty(StatisticsTimeIntervalPropName))
    prop.map(s => Try(s.toInt).getOrElse(DeafaultStatisticsTimeInterval)).getOrElse(DeafaultStatisticsTimeInterval).abs
  }
  
}