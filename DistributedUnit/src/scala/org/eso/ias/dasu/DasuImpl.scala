package org.eso.ias.dasu

import scala.util.Try
import java.util.concurrent.ScheduledFuture
import org.eso.ias.dasu.publisher.OutputPublisher
import org.eso.ias.dasu.subscriber.InputSubscriber
import org.eso.ias.cdb.CdbReader
import org.eso.ias.prototype.input.Identifier
import org.eso.ias.prototype.input.java.IdentifierType
import scala.collection.mutable.{Map => MutableMap}
import org.eso.ias.dasu.topology.Topology
import org.eso.ias.prototype.compele.ComputingElement
import scala.util.Properties
import org.eso.ias.cdb.pojos.AsceDao
import org.eso.ias.prototype.input.java.IASValue
import org.ias.prototype.logging.IASLogger
import java.util.concurrent.atomic.AtomicLong
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.atomic.AtomicReference
import java.util.Properties
import scala.util.Failure
import scala.util.Success
import scala.collection.JavaConverters
import org.eso.ias.prototype.input.InOut
import org.eso.ias.prototype.compele.AsceStates
import org.eso.ias.dasu.executorthread.ScheduledExecutor
import org.eso.ias.prototype.input.JavaConverter
import java.util.concurrent.TimeUnit
import org.eso.ias.cdb.pojos.DasuDao

/**
 * The implementation of the DASU
 * 
 * @param the identifier of the DASU
 * @param outputPublisher the publisher to send the output
 * @param inputSubscriber the subscriber getting events to be processed 
 * @param cdbReader the CDB reader to get the configuration of the DASU from the CDB
 * 
 */
class DasuImpl (
    dasuIdentifier: Identifier,
    private val outputPublisher: OutputPublisher,
    private val inputSubscriber: InputSubscriber,
    cdbReader: CdbReader)
    extends Dasu(dasuIdentifier) with Runnable {
  require(Option(dasuIdentifier).isDefined,"Invalid Identifier")
  require(dasuIdentifier.idType==IdentifierType.DASU,"Invalid identifier type for DASU")
  require(Option(outputPublisher).isDefined,"Invalid output publisher")
  require(Option(inputSubscriber).isDefined,"Invalid input subscriber")
  require(Option(cdbReader).isDefined,"Invalid CDB reader")
  
  /** The logger */
  private val logger = IASLogger.getLogger(this.getClass)
  
  logger.info("Building DASU [{}] with running id {}",id,dasuIdentifier.fullRunningID)
  
  // Read configuration from CDB
  val dasuDao = {
    val dasuOptional = cdbReader.getDasu(id)
    require(dasuOptional.isPresent(),"DASU ["+id+"] configuration not found on cdb")
    dasuOptional.get
  }
  // TODO: release CDB resources
  logger.debug("DASU [{}] configuration red from CDB",id)
  
  /**
   * The configuration of the ASCEs that run in the DASU
   */
  val asceDaos = JavaConverters.asScalaSet(dasuDao.getAsces).toList
  
  // Are there ASCEs assigned to this DASU?
  require(dasuDao.getAsces.size()>0,"No ASCE found for DASU "+id)
  logger.info("DASU [{}]: will load and run {} ASCEs",id,""+dasuDao.getAsces.size())
  
  // The ID of the output generated by the DASU
  val dasuOutputId = dasuDao.getOutput().getId()
  
  // Build the topology
  val dasuTopology: Topology = new Topology(
      id,
      dasuOutputId,
      asceDaos)
  logger.debug("DASU [{}]: topology built",id)
  logger.debug(dasuTopology.toString())
  
  // Instantiate the ASCEs
  val asces: Map[String, ComputingElement[_]] = {
    val addToMapFunc = (m: Map[String, ComputingElement[_]], asce: AsceDao) => {
      val propsForAsce = new Properties()
      asce.getProps.forEach(p => propsForAsce.setProperty(p.getName, p.getValue))
      m + (asce.getId -> ComputingElement(asce,dasuIdentifier,propsForAsce))
    }
    asceDaos.foldLeft(Map[String,ComputingElement[_]]())(addToMapFunc)
  }
  logger.info("DASU [{}] ASCEs loaded: [{}]",id, asces.keys.mkString(", "))
    
  // Activate the ASCEs
  val ascesInitedOk=asces.valuesIterator.map(asce => asce.initialize()).forall(s => s==AsceStates.InputsUndefined)
  assert(ascesInitedOk,"At least one ASCE did not pass the initialization phase")
  logger.info("DASU [{}]: ASCEs initialized",id)
  
  // The ASCE that produces the output of the DASU
  val asceThatProducesTheOutput = dasuTopology.asceProducingOutput(dasuOutputId)
  require(asceThatProducesTheOutput.isDefined && !asceThatProducesTheOutput.isEmpty,"ASCE producing output not found")
  logger.info("The output [{}] of the DASU [{}] is produced by [{}] ASCE",dasuOutputId,id,asceThatProducesTheOutput.get)
  
  // Get the required refresh rate i.e. the rate to produce the output 
  // of the DASU that is given by the ASCE that produces such IASIO
  val refreshRate = asceDaos.find(_.getOutput.getId==dasuOutputId).map(asce => asce.getOutput().getRefreshRate())
  require(refreshRate.isDefined,"Refresh rate not found!")
  logger.info("The DASU [{}] produces the output [{}] at a rate of {}ms",id,dasuOutputId,""+refreshRate.get)
  
  /**
   * Values received in input from plugins or other DASUs (BSDB)
   * and not yet processed
   */
  val notYetProcessedInputs: MutableMap[String,IASValue[_]] = MutableMap()
  
  /** The thread executor service */
  val scheduledExecutor = new ScheduledExecutor(id)
  
  /** 
   *  The helper to schedule the next refresh of the output
   *  when no inputs have been received
   */
  val timeScheduler: TimeScheduler = new TimeScheduler(dasuDao)
  
  /**
   * The point in time when the output has been generated
   * for the last time
   */
  val lastUpdateTime = new AtomicLong(Long.MinValue)
  
  /** True if the automatic refresh of the output has been enabled */
  val timelyRefreshing = new AtomicBoolean(false)
  
  /** The task that refreshes the output */
  val refreshTask: AtomicReference[ScheduledFuture[_]] = new AtomicReference[ScheduledFuture[_]]()
  
  logger.debug("DASU [{}] initializing the publisher", id)
  val outputPublisherInitialized = outputPublisher.initializePublisher()
  outputPublisherInitialized match {
    case Failure(f) => logger.error("DASU [{}] failed to initialize the publisher: NO output will be produced", id,f)
    case Success(s) => logger.info("DASU [{}] publisher successfully initialized",id)
  }
  logger.debug("DASU [{}] initializing the subscriber", id)
  
  val inputSubscriberInitialized = inputSubscriber.initializeSubscriber()
  inputSubscriberInitialized match {
    case Failure(f) => logger.error("DASU [{}] failed to initialize the subscriber: NO input will be processed", id,f)
    case Success(s) => logger.info("DASU [{}] subscriber successfully initialized",id)
  }
  
  logger.info("DASU [{}] built", id)
  
  /**
   * Propagates the inputs received from the BSDB to each of the ASCEs
   * in the DASU generating the output of the entire DASU.
   * 
   * THis method runs after the refresh time interval elapses.
   * All the iasios collected in the time interval will be passed to the first level of the ASCEs
   * and up till the last ASCE generates the output of the DASU itself.
   * Each ASCE runs the TF and produces another output to be propagated to the next level.
   * The last level is the only one ASCE that produces the output of the DASU
   * to be sent to the BSDB.
   * 
   * @param iasios the IASIOs received from the BDSB in the last time interval
   * @return the IASIO to send back to the BSDB
   */
  private def propagateIasios(iasios: Set[IASValue[_]]): Option[IASValue[_]] = {
      
      // Updates one ASCE i.e. runs its TF passing the inputs
      // Return the output of the ASCE
      def updateOneAsce(asceId: String, asceInputs: Set[IASValue[_]]): Option[IASValue[_]] = {
        
        val requiredInputs = dasuTopology.inputsOfAsce(asceId)
        assert(requiredInputs.isDefined,"No inputs required by "+asceId)
        val inputs: Set[IASValue[_]] = asceInputs.filter( p => requiredInputs.get.contains(p.id))
        // Get the ASCE with the given ID 
        val asceOpt = asces.get(asceId)
        assert(asceOpt.isDefined,"ASCE "+asceId+" NOT found!")
        val asce: ComputingElement[_] = asceOpt.get
        val asceOutputOpt: Option[InOut[_]] = asce.update(inputs)._1
        asceOutputOpt.map (inout => JavaConverter.inOutToIASValue(inout))
      }
      
      // Run the TFs of all the ASCEs in one level
      // Returns the inputs plus all the outputs produced by the ACSEs
      def updateOneLevel(asces: Set[String], levelInputs: Set[IASValue[_]]): Set[IASValue[_]] = {
        
        asces.foldLeft(levelInputs) ( 
          (s: Set[IASValue[_]], id: String ) => {
            val output = updateOneAsce(id, levelInputs) 
            if (output.isDefined) s+output.get
            else s})
      }
      
      val outputs = dasuTopology.levels.foldLeft(iasios){ (s: Set[IASValue[_]], ids: Set[String]) => s ++ updateOneLevel(ids, s) }
      
      // TODO: update the validity!!!
      outputs.find(_.id==dasuOutputId)
  }
  
  /**
   * Updates the output with the inputs received
   * 
   * @param iasios the inputs received
   * @see InputsListener
   */
  override def inputsReceived(iasios: Set[IASValue[_]]) = synchronized {
    
    // Merge the inputs with the buffered ones to keep ony the last update
    iasios.foreach(iasio => notYetProcessedInputs.put(iasio.id,iasio))
    if (!timelyRefreshing.get || // Send immediately 
        refreshTask.get().getDelay(TimeUnit.MILLISECONDS)<=0 || // Timer task
        // Or execute only if does not happen more often then the minAllowedRefreshRate:
        (
            // The delay from last execution is greater then minAllowedRefreshRate
            System.currentTimeMillis()-lastUpdateTime.get()>=timeScheduler.minAllowedRefreshRate &&
            // Next timer task will happen later then minAllowedRefreshRate
            refreshTask.get().getDelay(TimeUnit.MILLISECONDS)>timeScheduler.minAllowedRefreshRate
        )) {
      val before = System.currentTimeMillis()
      val newOutput = propagateIasios(notYetProcessedInputs.values.toSet)
      val after = System.currentTimeMillis()
      lastUpdateTime.set(after)
      notYetProcessedInputs.clear()
      // We ignore the next execution time provided by the TimeScheduler
      // with the disadvantage that the TimeScheduler takes into account
      // the average execution time to produce the output
      // We call getNextRefreshTime because it publish statistics 
      // that could help debugging
      timeScheduler.getNextRefreshTime(after-before)
      
      newOutput.foreach( output => {
        outputPublisher.publish(output) 
        logger.debug("DASU [{}]: output published",id)})
    }
  }
  
  /** 
   *  Start getting events from the inputs subscriber
   *  to produce the output
   */
  def start(): Try[Unit] = {
    inputSubscriberInitialized.map(x => inputSubscriber.startSubscriber(this, dasuTopology.dasuInputs))
  }
  
  /** The task to refresh the output when no new inputs have been received */
  override def run() = {
    inputsReceived(Set())
  }
  
  /** Cancel the timer task */
  private def cancelTimerTask() =  synchronized {
    val oldTask: Option[ScheduledFuture[_]] = Option(refreshTask.getAndSet(null))
    oldTask.foreach(task => task.cancel(false))
    
  }
  
  /**
   * Deactivate the automatic update of the output
   * in case no new inputs arrive.
   */
  def disableAutoRefreshOfOutput() = synchronized {
    val isEnabled = timelyRefreshing.getAndSet(false)
    if (isEnabled) {
      cancelTimerTask()
      logger.info("Dasu [{}]: automatic refresh of output disabled",id)
    }
  }
  
  /**
   * Activate the automatic update of the output
   * in case no new inputs arrive.
   * 
   * Most likely, the value of the output remains the same 
   * while the validity could change.
   */
  def enableAutoRefreshOfOutput() = synchronized {
    val alreadyActive = timelyRefreshing.getAndSet(true)
    if (alreadyActive) {
      logger.warn("DASU [{}]: automatic refresh of output already ative",id)
    } else {
      cancelTimerTask()
      val newTask=scheduledExecutor.scheduleWithFixedDelay(
          this, 
          timeScheduler.normalizedRefreshRate, 
          timeScheduler.normalizedRefreshRate,
          TimeUnit.MILLISECONDS)
      refreshTask.set(newTask)
      logger.info("Dasu [{}]: automatic refresh of output enabled at intervals of {} msecs",
          id,
          timeScheduler.normalizedRefreshRate.toString())
    }
  }
  
  /**
   * Release all the resources before exiting
   */
  def cleanUp() {
    logger.info("DASU [{}]: releasing resources", id)
    logger.debug("DASU [{}]: stopping the auto-refresh of the output", id)
    Try(disableAutoRefreshOfOutput())
    logger.debug("DASU [{}]: releasing the subscriber", id)
    Try(inputSubscriber.cleanUpSubscriber())
    logger.debug("DASU [{}]: releasing the publisher", id)
    Try(outputPublisher.cleanUpPublisher())
    logger.info("DASU [{}]: cleaned up",id)
  }
  
  /** The inputs of the DASU */
  def getInputIds(): Set[String] = dasuTopology.dasuInputs
  
  /** The IDs of the ASCEs running in the DASU  */
  def getAsceIds(): Set[String] = asces.keys.toSet
}

object DasuImpl {
  
  /**
   * Factory method to build a DasuImpl
   * 
   * @param dasuDao: the configuration of the DASU from the CDB
   * @param supervidentifier: the identifier of the supervisor that runs the dasu
   * @param outputPublisher: the producer to send outputs of DASUs to the BSDB
   * @param inputSubscriber: the consumer to get values from the BSDB
   * @param cdbReader: the CDB reader
   */
  def apply(
    dasuDao: DasuDao, 
    supervidentifier: Identifier, 
    outputPublisher: OutputPublisher,
    inputSubscriber: InputSubscriber,
    cdbReader: CdbReader): DasuImpl = {
    
    require(Option(dasuDao).isDefined)
    require(Option(supervidentifier).isDefined)
    require(Option(outputPublisher).isDefined)
    require(Option(inputSubscriber).isDefined)
    require(Option(cdbReader).isDefined)
   
    val dasuId = dasuDao.getId
    
    val dasuIdentifier = new Identifier(dasuId,IdentifierType.DASU,supervidentifier)
    
    new DasuImpl(dasuIdentifier,outputPublisher,inputSubscriber,cdbReader)
  }
}