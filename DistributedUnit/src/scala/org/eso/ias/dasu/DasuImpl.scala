package org.eso.ias.dasu

import scala.util.Try
import java.util.concurrent.ScheduledFuture
import org.eso.ias.dasu.publisher.OutputPublisher
import org.eso.ias.dasu.subscriber.InputSubscriber
import org.eso.ias.cdb.CdbReader
import org.eso.ias.types.Identifier
import org.eso.ias.types.IdentifierType
import scala.collection.mutable.{Map => MutableMap}
import org.eso.ias.dasu.topology.Topology
import org.eso.ias.prototype.compele.ComputingElement
import scala.util.Properties
import org.eso.ias.cdb.pojos.AsceDao
import org.eso.ias.types.IASValue
import org.ias.logging.IASLogger
import java.util.concurrent.atomic.AtomicLong
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.atomic.AtomicReference
import java.util.Properties
import scala.util.Failure
import scala.util.Success
import scala.collection.JavaConverters
import org.eso.ias.types.InOut
import org.eso.ias.prototype.compele.AsceStates
import org.eso.ias.dasu.executorthread.ScheduledExecutor
import org.eso.ias.types.JavaConverter
import java.util.concurrent.TimeUnit
import org.eso.ias.cdb.pojos.DasuDao
import org.eso.ias.types.Validity
import org.eso.ias.types.IasValidity

/**
 * The implementation of the DASU.
 * 
 * A DASU normally has 2 threads running:
 * - the automatic resending of the output when no new input arrives
 * - a thread for the throttling to avoid that processing too many 
 *   inputs takes 100% CPU
 *   
 * 
 * 
 * 
 * @param the identifier of the DASU
 * @param outputPublisher the publisher to send the output
 * @param inputSubscriber the subscriber getting events to be processed 
 * @param cdbReader the CDB reader to get the configuration of the DASU from the CDB
 * @param autoSendTimeInterval refresh rate (msec) to automatically send the output  
 *                             when no new inputs have been received
 */
class DasuImpl (
    dasuIdentifier: Identifier,
    private val outputPublisher: OutputPublisher,
    private val inputSubscriber: InputSubscriber,
    cdbReader: CdbReader,
    val autoSendTimeInterval: Long)
    extends Dasu(dasuIdentifier) {
  require(Option(dasuIdentifier).isDefined,"Invalid Identifier")
  require(dasuIdentifier.idType==IdentifierType.DASU,"Invalid identifier type for DASU")
  require(Option(outputPublisher).isDefined,"Invalid output publisher")
  require(Option(inputSubscriber).isDefined,"Invalid input subscriber")
  require(Option(cdbReader).isDefined,"Invalid CDB reader")
  require(Option(autoSendTimeInterval).isDefined && autoSendTimeInterval>0,"Invalid auto-send time interval")
  
  /** The logger */
  private val logger = IASLogger.getLogger(this.getClass)
  
  logger.info("Building DASU [{}] with running id {}",id,dasuIdentifier.fullRunningID)
  
  // Read configuration from CDB
  val dasuDao = {
    val dasuOptional = cdbReader.getDasu(id)
    require(dasuOptional.isPresent(),"DASU ["+id+"] configuration not found on cdb")
    dasuOptional.get
  }
  // TODO: release CDB resources
  logger.debug("DASU [{}] configuration red from CDB",id)
  
  /**
   * The configuration of the ASCEs that run in the DASU
   */
  val asceDaos = JavaConverters.asScalaSet(dasuDao.getAsces).toList
  
  // Are there ASCEs assigned to this DASU?
  require(dasuDao.getAsces.size()>0,"No ASCE found for DASU "+id)
  logger.info("DASU [{}]: will load and run {} ASCEs",id,""+dasuDao.getAsces.size())
  
  // The ID of the output generated by the DASU
  val dasuOutputId = dasuDao.getOutput().getId()
  
  // Build the topology
  val dasuTopology: Topology = new Topology(
      id,
      dasuOutputId,
      asceDaos)
  logger.debug("DASU [{}]: topology built",id)
  logger.debug(dasuTopology.toString())
  
  // Instantiate the ASCEs
  val asces: Map[String, ComputingElement[_]] = {
    val addToMapFunc = (m: Map[String, ComputingElement[_]], asce: AsceDao) => {
      val propsForAsce = new Properties()
      asce.getProps.forEach(p => propsForAsce.setProperty(p.getName, p.getValue))
      m + (asce.getId -> ComputingElement(asce,dasuIdentifier,propsForAsce))
    }
    asceDaos.foldLeft(Map[String,ComputingElement[_]]())(addToMapFunc)
  }
  logger.info("DASU [{}] ASCEs loaded: [{}]",id, asces.keys.mkString(", "))
    
  // Activate the ASCEs
  val ascesInitedOk=asces.valuesIterator.map(asce => asce.initialize()).forall(s => s==AsceStates.InputsUndefined)
  assert(ascesInitedOk,"At least one ASCE did not pass the initialization phase")
  logger.info("DASU [{}]: ASCEs initialized",id)
  
  // The ASCE that produces the output of the DASU
  val idOfAsceThatProducesTheOutput = {
    val idOpt=dasuTopology.asceProducingOutput(dasuOutputId)
    require(idOpt.isDefined && !idOpt.isEmpty,"ASCE producing output not found")
    idOpt.get
  }
  logger.info("The output [{}] of the DASU [{}] is produced by [{}] ASCE",dasuOutputId,id,idOfAsceThatProducesTheOutput)
  
  /** The ASCE that produces the output */
  val asceThatProducesTheOutput = asces(idOfAsceThatProducesTheOutput)
  
  // The refresh rate of the output
  // (it is the refresh rate of the IASIO of the ASCE that produces the output of the DASU)
  val outputRefreshRate = asceDaos.find(_.getOutput.getId==dasuOutputId).map(asce => asce.getOutput().getRefreshRate())
  require(outputRefreshRate.isDefined,"Refresh rate of the output not found!")
  
  /**
   * Values that have been received in input from plugins or other DASUs (BSDB)
   * and not yet processed
   * 
   * TODO: protect the map against access from multiple threads
   */
  val notYetProcessedInputs: MutableMap[String,IASValue[_]] = MutableMap()
  
  /** 
   *  The last calculated output
   */
  val lastCalculatedOutput = new AtomicReference[Option[IASValue[_]]](None)
  
  /** 
   *  The last sent output and validity
   *  this is sent again if no new inputs arrives and the autoSendTimerTask is running
   *  and the validity did not change since the last sending
   */
  val lastSentOutputAndValidity = new AtomicReference[Option[Tuple2[IASValue[_], IasValidity]]](None)
  
  /** 
   *  The point in time when the output has been sent to the
   *  BSDB either due to new inputs or auto-refresh
   *  
   *  It is better to initialize at the actual timestamp
   *  for the calculation of the throttling in inputsReceived 
   */
  val lastSentTime = new AtomicLong(System.currentTimeMillis())
  
  /** The thread executor service */
  val scheduledExecutor = new ScheduledExecutor(id)
  
  /** True if the automatic re-sending of the output has been enabled */
  val timelyRefreshing = new AtomicBoolean(false)
  
  /** True if the DASU has been started */
  val started = new AtomicBoolean(false)
  
  /** 
   *  The task to delay the generation the output 
   *  when new inputs must be processed 
   */
  val throttlingTask = new AtomicReference[Option[ScheduledFuture[_]]](None)
  
  /**
   * The Runnable to update the output when it is delayed by the throttling
   */
  val delayedUpdateTask = new Runnable {
      override def run() = {
        logger.info("DASU [{}] running the throttling task",id)
        updateAndPublishOutput()
      }
  }
  
  /**
   * The point in time of the last time when 
   * the output has been generated and published
   */
  val lastUpdateTime = new AtomicLong(0)
  
  /** 
   *  The task that timely send the last computed output
   *  when no new inputs arrived: initially disabled, must be enabled 
   *  invoking enableAutoRefreshOfOutput.
   *  
   *  If a new output is generated before this time interval elapses,
   *  this task is delayed of the duration of autoSendTimeInterval msecs.  
   */
  val autoSendTimerTask: AtomicReference[ScheduledFuture[_]] = new AtomicReference[ScheduledFuture[_]]()
  
  /** Closed: the DASU does not process inputs */
  val closed = new AtomicBoolean(false)
  
  logger.debug("DASU [{}]: initializing the publisher", id)
  
  val outputPublisherInitialized = outputPublisher.initializePublisher()
  outputPublisherInitialized match {
    case Failure(f) => logger.error("DASU [{}] failed to initialize the publisher: NO output will be produced", id,f)
    case Success(s) => logger.info("DASU [{}] publisher successfully initialized",id)
  }
  logger.debug("DASU [{}] initializing the subscriber", id)
  
  val inputSubscriberInitialized = inputSubscriber.initializeSubscriber()
  inputSubscriberInitialized match {
    case Failure(f) => logger.error("DASU [{}] failed to initialize the subscriber: NO input will be processed", id,f)
    case Success(s) => logger.info("DASU [{}] subscriber successfully initialized",id)
  }
  
  /** The generator of statistics */
  val statsCollector = new StatsCollector(id)
  
  logger.info("DASU [{}] built", id)
  
  /**
   * Reschedule the auto send time interval if the output is generated before
   * the autoSendTimeInterval elapses.
   */
  private def rescheduleAutoSendTimeInterval() = synchronized {
    val autoSendIsEnabled = timelyRefreshing.get()
    if (autoSendIsEnabled) {
      
      val lastTimerScheduledFeature: Option[ScheduledFuture[_]] = Option(autoSendTimerTask.get)
      lastTimerScheduledFeature.foreach(_.cancel(false))
      val runnable = new Runnable {
            /** The task to refresh the output when no new inputs have been received */
            override def run() = publishOutput(None)
        }
        val newTask=scheduledExecutor.scheduleWithFixedDelay(runnable, autoSendTimeInterval, autoSendTimeInterval, TimeUnit.MILLISECONDS)
        autoSendTimerTask.set(newTask)
    }
  }
  
  /**
   * Propagates the inputs received from the BSDB to each of the ASCEs
   * in the DASU generating the output of the entire DASU.
   * 
   * This method runs after the refresh time interval elapses.
   * All the iasios collected in the time interval will be passed to the first level of the ASCEs
   * and up till the last ASCE generates the output of the DASU itself.
   * Each ASCE runs the TF and produces another output to be propagated to the next level.
   * The last level is the only one ASCE that produces the output of the DASU
   * to be sent to the BSDB.
   * 
   * @param iasios the IASIOs received from the BDSB in the last time interval
   * @return the IASIO to send back to the BSDB
   */
  private def propagateIasios(iasios: Set[IASValue[_]]): Option[IASValue[_]] = {
      
      // Updates one ASCE i.e. runs its TF passing the inputs
      // Return the output of the ASCE
      def updateOneAsce(asceId: String, asceInputs: Set[IASValue[_]]): IASValue[_] = {
        
        val requiredInputs = dasuTopology.inputsOfAsce(asceId)
        assert(requiredInputs.isDefined,"No inputs required by "+asceId)
        val inputs: Set[IASValue[_]] = asceInputs.filter( p => requiredInputs.get.contains(p.id))
        // Get the ASCE with the given ID 
        val asceOpt = asces.get(asceId)
        assert(asceOpt.isDefined,"ASCE "+asceId+" NOT found!")
        val asce: ComputingElement[_] = asceOpt.get
        val updateRes: Tuple3[InOut[_],Validity, AsceStates.State] = asce.update(inputs)
        JavaConverter.inOutToIASValue(updateRes._1,updateRes._2)
      }
      
      // Run the TFs of all the ASCEs in one level
      // Returns the inputs plus all the outputs produced by the ACSEs
      def updateOneLevel(asces: Set[String], levelInputs: Set[IASValue[_]]): Set[IASValue[_]] = {
        
        asces.foldLeft(levelInputs) ( 
          (s: Set[IASValue[_]], id: String ) => {
            val output = updateOneAsce(id, levelInputs) 
            s+output})
      }
      
      if (!closed.get) {
        val outputs = dasuTopology.levels.foldLeft(iasios){ (s: Set[IASValue[_]], ids: Set[String]) => s ++ updateOneLevel(ids, s) }
        outputs.find(_.id==dasuOutputId)
      } else {
        None
      }
  }
  
  /**
   * Updates the output with new inputs have been received from the BSDB.
   * 
   * This method is not invoked while automatically re-sending the last computed value.
   * Such value is in fact stored into lastSentOutput and does not trigger
   * a recalculation by the DASU. 
   * 
   * @param iasios the inputs received
   * @see InputsListener
   */
  override def inputsReceived(iasios: Set[IASValue[_]]) = synchronized {
    assert(iasios.size>0)
        
    // Merge the inputs with the buffered ones to keep only the last updated values
    iasios.foreach(iasio => notYetProcessedInputs.put(iasio.id,iasio))
    
    // The new output must be immediately recalculated and sent unless 
    // * the throttling is already in place (i.e. calculation already delayed)
    // * the last value has been updated shortly before 
    //   (i.e. the calculation must be delayed and the throttling activated)
    val now = System.currentTimeMillis()
    val afterEndOfThrottling = now>lastUpdateTime.get+throttling
    val beforeEndOfThrottling = !afterEndOfThrottling
    val throttlingIsScheduled = throttlingTask.get().isDefined && !throttlingTask.get().get.isDone()
    
    (throttlingIsScheduled, beforeEndOfThrottling) match {
      case (true, true) =>  {}
                          // delayed: do nothing
      case (true, false) | (false, false) =>  
        updateAndPublishOutput() // send immediately
      case (false, true) => 
        val delay =  throttling+now-lastSentTime.get
        val schedFeature = scheduledExecutor.schedule(delayedUpdateTask,delay,TimeUnit.MILLISECONDS)
        throttlingTask.set(Some(schedFeature))
    }
  }
  
  /**
   * Publish the passed output to the BSDB.
   * 
   * The value effectively sent to the BSDB depends on the 
   * passed parameter and the last sent value.
   * 
   * The calculated output has the validity calculated when it has been produced;
   * if the calculatedoutput is empty, the validity must be updated as the
   * validity of one of the inputs could have been changed in the mean time 
   * 
   * @param calculatedOutput the last calculated output (it has the last validity)
   *                         it is empty if this method has been called
   *                         without calculating an output (automatic sending for example)
   */
  def publishOutput(calculatedOutput: Option[IASValue[_]]) {
    
    val valueAndValidityToSend: Option[Tuple2[IASValue[_],IasValidity]] = 
      ( calculatedOutput, lastSentOutputAndValidity.get) match {
        case ( None, Some(sentVal)) =>
          // Automatic sending of the last value
          Option(sentVal._1,asceThatProducesTheOutput.getOutput()._2.iasValidity)
        case ( Some(calc), _) =>
          // Newly calculated value takes precedence
          Option(calc, calc.iasValidity)
        case ( None, None ) => None
    }
    
    
    valueAndValidityToSend.foreach( valueAndValidity => {
      // Shall we update the validity?
      val valueToSendWithLastValidity= if (valueAndValidity._1.iasValidity==valueAndValidity._2) {
        valueAndValidity._1  
      } else {
        IASValue.buildIasValue(
            valueAndValidity._1.value, 
            System.currentTimeMillis(), 
            valueAndValidity._1.mode, 
            valueAndValidity._2, // Set the actual validity 
            valueAndValidity._1.fullRunningId, 
            valueAndValidity._1.valueType)
      }
      
      // Finally send the value but only if the value of the IASValue ios not null!
      Option(valueToSendWithLastValidity.value).foreach( _ => {
        outputPublisher.publish(valueToSendWithLastValidity)
        lastSentTime.set(System.currentTimeMillis())
        lastSentOutputAndValidity.set(Some((valueToSendWithLastValidity,valueAndValidity._2)))
        logger.debug("DASU [{}]: output published",id)  
      })
         
    })
  }
  
  /**
   * Update the output with the set of inputs collected so far
   * and send the output to the BSDB.
   * 
   * The method delegates the calculation to propapgateIasios
   */
  private def updateAndPublishOutput() = {
        
    val before = System.currentTimeMillis()
    lastCalculatedOutput.set(propagateIasios(notYetProcessedInputs.values.toSet))
    val after = System.currentTimeMillis()
    lastUpdateTime.set(after)
    notYetProcessedInputs.clear()
    publishOutput(lastCalculatedOutput.get) 
    rescheduleAutoSendTimeInterval()
    statsCollector.updateStats(after-before)
  }
  
  /** 
   *  Start getting events from the inputs subscriber
   *  to produce the output
   */
  def start(): Try[Unit] = {
    val alreadyStarted = started.getAndSet(true)
    if (!alreadyStarted) {
      logger.debug("DASU [{}] starting", id)
      inputSubscriberInitialized.map(x => inputSubscriber.startSubscriber(this, dasuTopology.dasuInputs))
    } else {
      new Failure(new Exception("DASU already started"))
    }
  }
  
  /**
   * Enable/disable the automatic update of the output
   * in case no new inputs arrive.
   * 
   * Most likely, the value of the output remains the same 
   * while the validity could change.
   */
  def enableAutoRefreshOfOutput(enable: Boolean) = synchronized {
    val alreadyEnabled = timelyRefreshing.getAndSet(enable)
    (enable, alreadyEnabled) match {
      case (true, true) => 
        logger.warn("DASU [{}]: automatic refresh of output already ative",id)
      case (true, false) => 
        rescheduleAutoSendTimeInterval()
        logger.info("DASU [{}]: automatic refresh of output enabled at intervals of {} msecs (aprox)",id, autoSendTimeInterval.toString())
      case (false , _) => 
        val oldTask: Option[ScheduledFuture[_]] = Option(autoSendTimerTask.getAndSet(null))
        oldTask.foreach(task => {
          task.cancel(false)
          logger.info("DASU [{}]: automatic refresh of output disaabled",id)  
        })
    }
  }
  
  /**
   * Release all the resources before exiting
   */
  def cleanUp() {
    val alreadyClosed = closed.getAndSet(true)
    if (alreadyClosed) {
      logger.warn("DASU [{}]: already cleaned up!", id)
    } else {
      logger.info("DASU [{}]: releasing resources", id)
      logger.debug("DASU [{}]: stopping the auto-refresh of the output", id)
      Try(enableAutoRefreshOfOutput(false))
      logger.debug("DASU [{}]: releasing the subscriber", id)
      Try(inputSubscriber.cleanUpSubscriber())
      logger.debug("DASU [{}]: releasing the publisher", id)
      Try(outputPublisher.cleanUpPublisher())
      logger.info("DASU [{}]: cleaned up",id)  
    }
  }
  
  /** The inputs of the DASU */
  def getInputIds(): Set[String] = dasuTopology.dasuInputs
  
  /** The IDs of the ASCEs running in the DASU  */
  def getAsceIds(): Set[String] = asces.keys.toSet
}

object DasuImpl {
  
  /**
   * Factory method to build a DasuImpl
   * 
   * @param dasuDao: the configuration of the DASU from the CDB
   * @param supervidentifier: the identifier of the supervisor that runs the dasu
   * @param outputPublisher: the producer to send outputs of DASUs to the BSDB
   * @param inputSubscriber: the consumer to get values from the BSDB
   * @param cdbReader: the CDB reader
   * @param autoSendTimeInterval refresh rate (msec) to automatically send the output  
   *                             when no new inputs have been received
   */
  def apply(
    dasuDao: DasuDao, 
    supervidentifier: Identifier, 
    outputPublisher: OutputPublisher,
    inputSubscriber: InputSubscriber,
    cdbReader: CdbReader,
    autoSendTimeInterval: Long): DasuImpl = {
    
    require(Option(dasuDao).isDefined)
    require(Option(supervidentifier).isDefined)
    require(Option(outputPublisher).isDefined)
    require(Option(inputSubscriber).isDefined)
    require(Option(cdbReader).isDefined)
    require(Option(autoSendTimeInterval).isDefined)
   
    val dasuId = dasuDao.getId
    
    val dasuIdentifier = new Identifier(dasuId,IdentifierType.DASU,supervidentifier)
    
    new DasuImpl(dasuIdentifier,outputPublisher,inputSubscriber,cdbReader,autoSendTimeInterval)
  }
}
